{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b4b404",
   "metadata": {},
   "source": [
    "# Модель DeepLabV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfab9358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from typing import Dict, Any\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52944bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.transform = transform\n",
    "        self.images = sorted(list(self.image_dir.glob(\"*.png\"))) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        mask_path = self.mask_dir / img_path.name  # Маска с таким же именем\n",
    "\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"))  # Маска в градациях серого\n",
    "        mask = (mask > 0).astype(np.float32)  # Бинарная маска (0 или 1)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "        \n",
    "        if len(mask.shape) == 2:  # If mask is (256, 256), add channel dimension\n",
    "            mask = mask[None, ...]  # Shape: (1, 256, 256)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Аугментация\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    A.ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    A.ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ec31d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность изображений: torch.Size([4, 3, 256, 256])\n",
      "Размерность масок: torch.Size([4, 1, 256, 256])\n",
      "Тип изображений: torch.float32\n",
      "Тип масок: torch.float32\n",
      "Классы: tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SegmentationDataset(\n",
    "    image_dir=r'D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\train\\images',\n",
    "    mask_dir=r'D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\train\\masks',\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "valid_dataset = SegmentationDataset(\n",
    "    image_dir=r'D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\val\\images',\n",
    "    mask_dir=r'D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\val\\masks',\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "test_dataset = SegmentationDataset(\n",
    "    image_dir=r'D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\test\\images',\n",
    "    mask_dir=r'D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\test\\masks',\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                                batch_size=4,\n",
    "                                shuffle=True\n",
    "                                )\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                                batch_size=4\n",
    "                                )\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                                batch_size=4\n",
    "                                )\n",
    "\n",
    "images, masks = next(iter(train_loader))\n",
    "print(\"Размерность изображений:\", images.shape)\n",
    "print(\"Размерность масок:\", masks.shape)\n",
    "print(\"Тип изображений:\", images.dtype)\n",
    "print(\"Тип масок:\", masks.dtype)\n",
    "print(\"Классы:\", masks.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e2ad6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchmetrics\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class DeepLabV3PlusLightning(pl.LightningModule):\n",
    "    def __init__(self, encoder_name=\"resnet101\", learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = smp.DeepLabV3Plus(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=None,\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None,\n",
    "        )\n",
    "        self.criterion = nn.BCEWithLogitsLoss()  # Лосс для бинарной сегментации\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Инициализация метрик\n",
    "        self.train_iou = torchmetrics.JaccardIndex(task=\"binary\", num_classes=2)\n",
    "        self.val_iou = torchmetrics.JaccardIndex(task=\"binary\", num_classes=2)\n",
    "        self.test_iou = torchmetrics.JaccardIndex(task=\"binary\", num_classes=2)\n",
    "        \n",
    "        self.train_accuracy = torchmetrics.Accuracy(task=\"binary\", threshold=0.5)\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task=\"binary\", threshold=0.5)\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task=\"binary\", threshold=0.5)\n",
    "        \n",
    "        self.train_precision = torchmetrics.Precision(task=\"binary\", threshold=0.5)\n",
    "        self.val_precision = torchmetrics.Precision(task=\"binary\", threshold=0.5)\n",
    "        self.test_precision = torchmetrics.Precision(task=\"binary\", threshold=0.5)\n",
    "        \n",
    "        self.train_recall = torchmetrics.Recall(task=\"binary\", threshold=0.5)\n",
    "        self.val_recall = torchmetrics.Recall(task=\"binary\", threshold=0.5)\n",
    "        self.test_recall = torchmetrics.Recall(task=\"binary\", threshold=0.5)\n",
    "        \n",
    "        self.train_f1 = torchmetrics.F1Score(task=\"binary\", threshold=0.5)\n",
    "        self.val_f1 = torchmetrics.F1Score(task=\"binary\", threshold=0.5)\n",
    "        self.test_f1 = torchmetrics.F1Score(task=\"binary\", threshold=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _shared_step(self, batch, stage):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, masks.float())\n",
    "        \n",
    "        # Применяем сигмоиду и порог для получения бинарных предсказаний\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        \n",
    "        metrics = {\n",
    "            f\"{stage}_loss\": loss,\n",
    "            f\"{stage}_iou\": getattr(self, f\"{stage}_iou\")(preds, masks.int()),\n",
    "            f\"{stage}_accuracy\": getattr(self, f\"{stage}_accuracy\")(preds, masks.int()),\n",
    "            f\"{stage}_precision\": getattr(self, f\"{stage}_precision\")(preds, masks.int()),\n",
    "            f\"{stage}_recall\": getattr(self, f\"{stage}_recall\")(preds, masks.int()),\n",
    "            f\"{stage}_f1\": getattr(self, f\"{stage}_f1\")(preds, masks.int()),\n",
    "        }\n",
    "        \n",
    "        # Логируем метрики\n",
    "        self.log_dict(metrics, prog_bar=True if stage == \"val\" else False)\n",
    "        \n",
    "        return loss if stage != \"test\" else metrics\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"test\")\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # Сбрасываем метрики в конце эпохи\n",
    "        self.train_iou.reset()\n",
    "        self.train_accuracy.reset()\n",
    "        self.train_precision.reset()\n",
    "        self.train_recall.reset()\n",
    "        self.train_f1.reset()\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # Сбрасываем метрики в конце эпохи\n",
    "        self.val_iou.reset()\n",
    "        self.val_accuracy.reset()\n",
    "        self.val_precision.reset()\n",
    "        self.val_recall.reset()\n",
    "        self.val_f1.reset()\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        # Сбрасываем метрики в конце эпохи\n",
    "        self.test_iou.reset()\n",
    "        self.test_accuracy.reset()\n",
    "        self.test_precision.reset()\n",
    "        self.test_recall.reset()\n",
    "        self.test_f1.reset()\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            patience=3, \n",
    "            factor=0.5,\n",
    "            verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8060b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "   | Name            | Type               | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0  | model           | DeepLabV3Plus      | 45.7 M | train\n",
      "1  | criterion       | BCEWithLogitsLoss  | 0      | train\n",
      "2  | train_iou       | BinaryJaccardIndex | 0      | train\n",
      "3  | val_iou         | BinaryJaccardIndex | 0      | train\n",
      "4  | test_iou        | BinaryJaccardIndex | 0      | train\n",
      "5  | train_accuracy  | BinaryAccuracy     | 0      | train\n",
      "6  | val_accuracy    | BinaryAccuracy     | 0      | train\n",
      "7  | test_accuracy   | BinaryAccuracy     | 0      | train\n",
      "8  | train_precision | BinaryPrecision    | 0      | train\n",
      "9  | val_precision   | BinaryPrecision    | 0      | train\n",
      "10 | test_precision  | BinaryPrecision    | 0      | train\n",
      "11 | train_recall    | BinaryRecall       | 0      | train\n",
      "12 | val_recall      | BinaryRecall       | 0      | train\n",
      "13 | test_recall     | BinaryRecall       | 0      | train\n",
      "14 | train_f1        | BinaryF1Score      | 0      | train\n",
      "15 | val_f1          | BinaryF1Score      | 0      | train\n",
      "16 | test_f1         | BinaryF1Score      | 0      | train\n",
      "----------------------------------------------------------------\n",
      "45.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "45.7 M    Total params\n",
      "182.679   Total estimated model params size (MB)\n",
      "359       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  27%|██▋       | 293/1088 [01:44<04:44,  2.79it/s, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:293\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    292\u001b[39m     batch = trainer.lightning_module._on_before_batch_transfer(batch, dataloader_idx=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     batch = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_to_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_ready()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:278\u001b[39m, in \u001b[36mStrategy.batch_to_device\u001b[39m\u001b[34m(self, batch, device, dataloader_idx)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply_batch_transfer_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m move_data_to_device(batch, device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:352\u001b[39m, in \u001b[36mLightningModule._apply_batch_transfer_handler\u001b[39m\u001b[34m(self, batch, device, dataloader_idx)\u001b[39m\n\u001b[32m    351\u001b[39m device = device \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransfer_batch_to_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m batch = \u001b[38;5;28mself\u001b[39m._call_batch_hook(\u001b[33m\"\u001b[39m\u001b[33mon_after_batch_transfer\u001b[39m\u001b[33m\"\u001b[39m, batch, dataloader_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:341\u001b[39m, in \u001b[36mLightningModule._call_batch_hook\u001b[39m\u001b[34m(self, hook_name, *args)\u001b[39m\n\u001b[32m    339\u001b[39m         trainer_method = call._call_lightning_datamodule_hook\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m hook = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, hook_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\core\\hooks.py:611\u001b[39m, in \u001b[36mDataHooks.transfer_batch_to_device\u001b[39m\u001b[34m(self, batch, device, dataloader_idx)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[33;03mstructure.\u001b[39;00m\n\u001b[32m    567\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    609\u001b[39m \n\u001b[32m    610\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmove_data_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\lightning_fabric\\utilities\\apply_func.py:110\u001b[39m, in \u001b[36mmove_data_to_device\u001b[39m\u001b[34m(batch, device)\u001b[39m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_TransferableDataType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_to\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\lightning_utilities\\core\\apply_func.py:68\u001b[39m, in \u001b[36mapply_to_collection\u001b[39m\u001b[34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous list\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous tuple\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\lightning_fabric\\utilities\\apply_func.py:104\u001b[39m, in \u001b[36mmove_data_to_device.<locals>.batch_to\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    103\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mnon_blocking\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m data_output = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     18\u001b[39m trainer = pl.Trainer(\n\u001b[32m     19\u001b[39m     max_epochs=\u001b[32m50\u001b[39m,  \u001b[38;5;66;03m# Количество эпох\u001b[39;00m\n\u001b[32m     20\u001b[39m     accelerator=\u001b[33m\"\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Использовать GPU, если доступно\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     log_every_n_steps=\u001b[32m10\u001b[39m,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# Инициализация модели\n",
    "model = DeepLabV3PlusLightning(encoder_name=\"resnet101\", learning_rate=1e-3)\n",
    "\n",
    "\n",
    "# Настройка логгера TensorBoard\n",
    "logger = TensorBoardLogger(save_dir=\"../data/logs/\", name=\"DeepLab_segmentation\")\n",
    "\n",
    "# Настройка callback для сохранения лучшей модели\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  \n",
    "    dirpath=\"../data/checkpoints/\", \n",
    "    filename=\"deeplab-{epoch:02d}-{val_loss:.2f}\",  \n",
    "    save_top_k=1, \n",
    "    mode=\"min\"  \n",
    ")\n",
    "\n",
    "# Инициализация тренера\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Количество эпох\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",  # Использовать GPU, если доступно\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "trainer.fit(model, train_loader, valid_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467dbbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование модели\n",
    "trainer.test(model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6728a43",
   "metadata": {},
   "source": [
    "# Инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab526eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели из чекпоинта\n",
    "model = DeepLabV3PlusLightning.load_from_checkpoint(\"deeplab_model.ckpt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Путь к файлам\n",
    "path = \"/input_images\"\n",
    "files = glob.glob(path)\n",
    "files = [file for file in files if os.path.isfile(file)]\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, image_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize([256, 256]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image, self.image_paths[idx]  # Возвращаем также путь к файлу\n",
    "\n",
    "# Создание DataLoader\n",
    "test_dataset = InferenceDataset(files)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Инференс с использованием trainer\n",
    "predictions = trainer.predict(model, dataloaders=test_loader)\n",
    "\n",
    "# Визуализация предсказанных масок\n",
    "plt.imshow(predictions[0], cmap=\"gray\")  \n",
    "plt.title(\"Segmentation Mask\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Создание директории для сохранения масок, если её нет\n",
    "output_dir = \"/output_masks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Обработка предсказаний и сохранение масок\n",
    "for batch, image_paths in zip(predictions, test_loader):\n",
    "    pred = batch  # Предсказания для батча\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    pred = pred.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Получаем путь к исходному изображению\n",
    "    image_path = image_paths[1][0]  # Путь к файлу из датасета\n",
    "    \n",
    "    # Формируем имя выходного файла\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_path = os.path.join(output_dir, f\"{base_name}_mask.png\")\n",
    "    \n",
    "    # Сохранение предсказанной маски\n",
    "    output_image = (pred * 255).astype(np.uint8)\n",
    "    output_pil = Image.fromarray(output_image)\n",
    "    output_pil.save(output_path)\n",
    "    print(f\"Saved mask: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vkr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
