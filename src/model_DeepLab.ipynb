{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b4b404",
   "metadata": {},
   "source": [
    "# Модель DeepLabV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfab9358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from typing import Dict, Any\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52944bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.transform = transform\n",
    "        self.images = sorted(list(self.image_dir.glob(\"*.png\"))) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        mask_path = self.mask_dir / img_path.name  # Маска с таким же именем\n",
    "\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"))  # Маска в градациях серого\n",
    "        mask = (mask > 0).astype(np.float32)  # Бинарная маска (0 или 1)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "        \n",
    "        if len(mask.shape) == 2:  # If mask is (256, 256), add channel dimension\n",
    "            mask = mask[None, ...]  # Shape: (1, 256, 256)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Аугментация\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    A.ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    A.ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81ec31d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность изображений: torch.Size([4, 3, 256, 256])\n",
      "Размерность масок: torch.Size([4, 1, 256, 256])\n",
      "Тип изображений: torch.float32\n",
      "Тип масок: torch.float32\n",
      "Классы: tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SegmentationDataset(\n",
    "    image_dir=r'D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\train\\images',\n",
    "    mask_dir=r'D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\train\\masks',\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "valid_dataset = SegmentationDataset(\n",
    "    image_dir=r'D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\val\\images',\n",
    "    mask_dir=r'D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\val\\masks',\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "test_dataset = SegmentationDataset(\n",
    "    image_dir=r'D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\test\\images',\n",
    "    mask_dir=r'D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\test\\masks',\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                                batch_size=4,\n",
    "                                shuffle=True\n",
    "                                )\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                                batch_size=4\n",
    "                                )\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                                batch_size=4\n",
    "                                )\n",
    "\n",
    "images, masks = next(iter(train_loader))\n",
    "print(\"Размерность изображений:\", images.shape)\n",
    "print(\"Размерность масок:\", masks.shape)\n",
    "print(\"Тип изображений:\", images.dtype)\n",
    "print(\"Тип масок:\", masks.dtype)\n",
    "print(\"Классы:\", masks.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e2ad6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchmetrics\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class DeepLabV3PlusLightning(pl.LightningModule):\n",
    "    def __init__(self, encoder_name=\"resnet50\", learning_rate=1e-3, encoder_weights='imagenet'):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = smp.DeepLabV3Plus(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None,\n",
    "        )\n",
    "        self.criterion = nn.BCEWithLogitsLoss()  # Лосс для бинарной сегментации\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Инициализация метрик\n",
    "        self.train_iou = torchmetrics.JaccardIndex(task=\"binary\", num_classes=2)\n",
    "        self.val_iou = torchmetrics.JaccardIndex(task=\"binary\", num_classes=2)\n",
    "        self.test_iou = torchmetrics.JaccardIndex(task=\"binary\", num_classes=2)\n",
    "        \n",
    "        self.train_accuracy = torchmetrics.Accuracy(task=\"binary\", num_classes=2)\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task=\"binary\", num_classes=2)\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task=\"binary\", num_classes=2)\n",
    "        \n",
    "        self.train_precision = torchmetrics.Precision(task=\"binary\", num_classes=2)\n",
    "        self.val_precision = torchmetrics.Precision(task=\"binary\", num_classes=2)\n",
    "        self.test_precision = torchmetrics.Precision(task=\"binary\", num_classes=2)\n",
    "        \n",
    "        self.train_recall = torchmetrics.Recall(task=\"binary\", num_classes=2)\n",
    "        self.val_recall = torchmetrics.Recall(task=\"binary\", num_classes=2)\n",
    "        self.test_recall = torchmetrics.Recall(task=\"binary\", num_classes=2)\n",
    "        \n",
    "        self.train_f1 = torchmetrics.F1Score(task=\"binary\", num_classes=2)\n",
    "        self.val_f1 = torchmetrics.F1Score(task=\"binary\", num_classes=2)\n",
    "        self.test_f1 = torchmetrics.F1Score(task=\"binary\", num_classes=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _shared_step(self, batch, stage):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, masks.float())\n",
    "        \n",
    "        # Применяем сигмоиду и порог для получения бинарных предсказаний\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        \n",
    "        metrics = {\n",
    "            f\"{stage}_loss\": loss,\n",
    "            f\"{stage}_iou\": getattr(self, f\"{stage}_iou\")(preds, masks.float()),\n",
    "            f\"{stage}_accuracy\": getattr(self, f\"{stage}_accuracy\")(preds, masks.float()),\n",
    "            f\"{stage}_precision\": getattr(self, f\"{stage}_precision\")(preds, masks.float()),\n",
    "            f\"{stage}_recall\": getattr(self, f\"{stage}_recall\")(preds, masks.float()),\n",
    "            f\"{stage}_f1\": getattr(self, f\"{stage}_f1\")(preds, masks.float()),\n",
    "        }\n",
    "        \n",
    "        # Логируем метрики\n",
    "        self.log_dict(metrics, prog_bar=True if stage == \"val\" else False)\n",
    "        \n",
    "        return loss if stage != \"test\" else metrics\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"test\")\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # Сбрасываем метрики в конце эпохи\n",
    "        self.train_iou.reset()\n",
    "        self.train_accuracy.reset()\n",
    "        self.train_precision.reset()\n",
    "        self.train_recall.reset()\n",
    "        self.train_f1.reset()\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # Сбрасываем метрики в конце эпохи\n",
    "        self.val_iou.reset()\n",
    "        self.val_accuracy.reset()\n",
    "        self.val_precision.reset()\n",
    "        self.val_recall.reset()\n",
    "        self.val_f1.reset()\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        # Сбрасываем метрики в конце эпохи\n",
    "        self.test_iou.reset()\n",
    "        self.test_accuracy.reset()\n",
    "        self.test_precision.reset()\n",
    "        self.test_recall.reset()\n",
    "        self.test_f1.reset()\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            patience=3, \n",
    "            factor=0.5,\n",
    "            verbose=True, # type: ignore\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8060b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory D:\\URFU\\VKR\\Ind_pract\\dissert\\data\\checkpoints exists and is not empty.\n",
      "Restoring states from the checkpoint path at ../data/checkpoints/deeplab-epoch=38-val_loss=0.19.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "   | Name            | Type               | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0  | model           | DeepLabV3Plus      | 26.7 M | train\n",
      "1  | criterion       | BCEWithLogitsLoss  | 0      | train\n",
      "2  | train_iou       | BinaryJaccardIndex | 0      | train\n",
      "3  | val_iou         | BinaryJaccardIndex | 0      | train\n",
      "4  | test_iou        | BinaryJaccardIndex | 0      | train\n",
      "5  | train_accuracy  | BinaryAccuracy     | 0      | train\n",
      "6  | val_accuracy    | BinaryAccuracy     | 0      | train\n",
      "7  | test_accuracy   | BinaryAccuracy     | 0      | train\n",
      "8  | train_precision | BinaryPrecision    | 0      | train\n",
      "9  | val_precision   | BinaryPrecision    | 0      | train\n",
      "10 | test_precision  | BinaryPrecision    | 0      | train\n",
      "11 | train_recall    | BinaryRecall       | 0      | train\n",
      "12 | val_recall      | BinaryRecall       | 0      | train\n",
      "13 | test_recall     | BinaryRecall       | 0      | train\n",
      "14 | train_f1        | BinaryF1Score      | 0      | train\n",
      "15 | val_f1          | BinaryF1Score      | 0      | train\n",
      "16 | test_f1         | BinaryF1Score      | 0      | train\n",
      "----------------------------------------------------------------\n",
      "26.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.7 M    Total params\n",
      "106.710   Total estimated model params size (MB)\n",
      "223       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at ../data/checkpoints/deeplab-epoch=38-val_loss=0.19.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46:   1%|          | 10/1088 [00:01<02:14,  8.04it/s, v_num=16, val_loss=0.193, val_iou=0.461, val_accuracy=0.928, val_precision=0.689, val_recall=0.579, val_f1=0.608]  "
     ]
    }
   ],
   "source": [
    "# Инициализация модели\n",
    "model = DeepLabV3PlusLightning(encoder_name=\"resnet50\", learning_rate=1e-3)\n",
    "\n",
    "\n",
    "# Настройка логгера TensorBoard\n",
    "logger = TensorBoardLogger(save_dir=\"../logs/\", name=\"DeepLab_segmentation\")\n",
    "\n",
    "# Настройка callback для сохранения лучшей модели\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  \n",
    "    dirpath=\"../data/checkpoints/\", \n",
    "    filename=\"deeplab-{epoch:02d}-{val_loss:.2f}\",  \n",
    "    save_top_k=1,\n",
    "    save_last=True, \n",
    "    mode=\"min\"  \n",
    ")\n",
    "\n",
    "# Инициализация тренера\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Количество эпох\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",  # Использовать GPU, если доступно\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "# Дообучение модели\n",
    "\n",
    "checkpoint_path = \"../data/checkpoints/deeplab-epoch=38-val_loss=0.19.ckpt\"\n",
    "\n",
    "trainer.fit(model, train_loader, valid_loader, ckpt_path=checkpoint_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467dbbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\ProgramData\\anaconda3\\envs\\vkr\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 234/234 [00:26<00:00,  8.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8936574459075928     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3096579611301422     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.20908024907112122    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3056327700614929     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6039301753044128     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2505630850791931     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8936574459075928    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3096579611301422    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.20908024907112122   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3056327700614929    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6039301753044128    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2505630850791931    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.3056327700614929,\n",
       "  'test_iou': 0.20908024907112122,\n",
       "  'test_accuracy': 0.8936574459075928,\n",
       "  'test_precision': 0.6039301753044128,\n",
       "  'test_recall': 0.2505630850791931,\n",
       "  'test_f1': 0.3096579611301422}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тестирование модели\n",
    "trainer.test(model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6728a43",
   "metadata": {},
   "source": [
    "# Инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab526eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели из чекпоинта\n",
    " \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_path = r\"D:\\URFU\\VKR\\Ind_pract\\dissert\\input\"\n",
    "files = glob.glob(os.path.join(input_path, \"*\"))\n",
    "files = [file for file in files if os.path.isfile(file)]\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, image_paths):\n",
    "        self.image_paths = image_paths  # Store paths here\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize([256, 256]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image \n",
    "\n",
    "# Загрузчик данных\n",
    "test_dataset = InferenceDataset(files)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "image_paths = test_dataset.image_paths \n",
    "\n",
    "# Инференс\n",
    "predictions = trainer.predict(model, dataloaders=test_loader)\n",
    "\n",
    "# Выгрузка\n",
    "output_dir = \"../output/deeplab\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Обработка файлов \n",
    "for i, pred_batch in enumerate(predictions): # type: ignore\n",
    "    \n",
    "    pred = pred_batch[0]\n",
    "    \n",
    "    # Преобразование в маску\n",
    "    prob = torch.sigmoid(pred)\n",
    "    mask = (prob > 0.5).float().squeeze().cpu().numpy() \n",
    "    \n",
    "    # Формирование путей выгрузки \n",
    "    path = image_paths[i]\n",
    "    base_name = os.path.splitext(os.path.basename(path))[0]\n",
    "    output_path = os.path.join(output_dir, f\"{base_name}_mask.png\")\n",
    "    \n",
    "    # Сохранение\n",
    "    Image.fromarray((mask * 255).astype(np.uint8)).save(output_path)\n",
    "    print(f\"Saved mask: {output_path}\")\n",
    "\n",
    "    plt.imshow(mask, cmap=\"gray\")\n",
    "    plt.title(f\"Segmentation Mask: {os.path.basename(path)}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vkr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
